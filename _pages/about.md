---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello, I am **Yichi Zhang** 张亦弛, an undergraduate student majoring in **Information Engineering** at the College of Information Science and Electronic Engineering, Zhejiang University. I am also minoring in **Computer Science and Technology** at the College of Computer Science and Technology, Zhejiang University.  

Previously, my main research direction was in **image/video generation and restoration**, under the guidance of Professor Xu. The recent emergence of models such as Stable Diffusion in the field of computer vision, and language models like ChatGPT in the field of natural language processing, has prompted me to focus on researching the **robustness and attack-defense of large models** represented by GPT or Diffusion Models under the guidance of Professor Xu and Professor Ji, and examining whether such models truly have robustness. 

Robustness & Sercurity in Machine Learning
======
The robustness issue of large models represented by GPT refers to whether these models can maintain stable performance when facing unknown or abnormal inputs, without becoming overconfident or crashing. These models are typically constructed based on deep learning neural networks, with millions to billions of parameters.

However, the robustness issue of these large models remains a challenge. On the one hand, these models are usually trained on large-scale datasets, but in practical applications, issues such as imbalanced data distribution, noise interference, and adversarial samples are often encountered, which may cause the model's performance to decline. On the other hand, some studies have shown that the overconfidence and vulnerability to adversarial attacks of these large models can cause security risks.

What I am doing now
======
With the increasing number of examples of AIG-generated fake news, I have started to pay attention to how to identify the content generated by AIG in a more novel and effective way. When GANs first appeared, we found that their generated quality was lower in high-frequency modules, so we could detect whether an image was GAN-generated by focusing on high-frequency modules. Today, we may need to look for features in the generated output of Diffusion Models to address this issue.

Injecting backdoor attacks into pre-trained models is no longer a rare occurrence, but in the fine-tuning process for downstream tasks, the injected backdoors are often overwritten, resulting in the failure of the attack. In fact, I am looking for a sustainable backdoor attack method to achieve continuous attacks.

Attacks against the same type of model often achieve successful results, but in practical applications, we often do not know exactly what type of model we are attacking. Therefore, I believe that research should be conducted on whether different attacks are transferable to different models. After this research is conducted, I may develop an attack method that is effective across different models.