---
title: 'The Reading of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
date: 2023-05-10
permalink: /posts/2023/05/blog-post-4
tags:
  - Transformer
  - BERT
  - Fine-tuning
---

There are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. In this paper, the authors introduced BERT(Bidirectional Encoder Representations from Transformers), which improved upon fine-tuning based approaches.

## Model Architecture

BERTâ€™s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.

To enable BERT to adapt well to downstream tasks, BERT's input representation needs to be able to represent both single and multiple sentences. BERT uses WordPiece embeddings, with the first token of each sentence being a special classification token [CLS], and different sentences being separated by [SEP]. A learnable embedding is added to each token to distinguish which sentence it belongs to.

![input](/images/2023/05/post4/pic0.png)

## Training

![train](/images/2023/05/post4/pic1.png)

BERT training consists of two stages: **pre-training** and **fine-tuning**.

During pre-training, BERT is trained on large-scale, unlabeled text corpora to learn general language features and patterns. BERT uses two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). The MLM task randomly masks some words in the input text and asks the model to predict their values, allowing the model to learn contextual information and semantic representations for the words. The NSP task requires the model to predict whether two sentences are adjacent in the original text, enabling the model to learn contextual relationships and information between sentences. During pre-training, BERT can be trained on large amounts of unlabeled data, such as Wikipedia, news articles, and web text.

During fine-tuning, the pre-trained model parameters are applied to a specific downstream task and further refined. In this stage, the parameters of the BERT model are used to initialize the parameters of the downstream task model, which is then fine-tuned on the specific task. Fine-tuning typically requires only a small amount of labeled data, since the pre-trained model has already learned general language features and patterns. Fine-tuning can be applied to a variety of downstream tasks, such as text classification, named entity recognition, and sentiment analysis.

## Experiments

BERT's performance was tested on various NLP tasks, the impact of different modules on BERT's performance was evaluated, and the effect of BERT model parameters on different NLP tasks was analyzed.